#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æŠ–éŸ³ä¸‹è½½å™¨ v2.0 - å¢å¼ºç‰ˆ
åŸºäºç ”ç©¶æˆæœä¼˜åŒ–ï¼Œæä¾›æ›´å‹å¥½çš„ä½¿ç”¨æ–¹å¼
æ”¯æŒå¤šç§cookieè·å–æ–¹å¼å’ŒURLç±»å‹
"""

import asyncio
import json
import logging
import os
import re
import sys
import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from urllib.parse import urlparse, parse_qs
import argparse
import yaml

# ç¬¬ä¸‰æ–¹åº“
try:
    import aiohttp
    import requests
    from rich.console import Console
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn, TaskProgressColumn
    from rich.table import Table
    from rich.panel import Panel
    from rich.prompt import Prompt, Confirm
    from rich import print as rprint
except ImportError as e:
    print(f"è¯·å®‰è£…å¿…è¦çš„ä¾èµ–: pip install aiohttp requests rich pyyaml")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# é…ç½®æ—¥å¿—ï¼ˆéœ€è¦åœ¨å¯¼å…¥æ¨¡å—ä¹‹å‰ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('downloader_v2.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from apiproxy.douyin import douyin_headers
from apiproxy.douyin.urls import Urls
from apiproxy.douyin.result import Result
from apiproxy.common.utils import Utils
from apiproxy.douyin.auth.cookie_manager import AutoCookieManager
from apiproxy.douyin.auth.signature_generator import get_x_bogus, get_a_bogus
from apiproxy.douyin.database import DataBase
from apiproxy.douyin.core.download_logger import DownloadLogger

# å°è¯•å¯¼å…¥browser_cookiesï¼Œå¦‚æœå¤±è´¥åˆ™è®¾ç½®ä¸ºNone
try:
    from apiproxy.douyin.auth.browser_cookies import get_browser_cookies
    HAS_BROWSER_COOKIES = True
except (ImportError, OSError) as e:
    logger.warning(f"æ— æ³•å¯¼å…¥browser_cookiesæ¨¡å—: {e}")
    logger.warning("æµè§ˆå™¨Cookieæå–åŠŸèƒ½ä¸å¯ç”¨")
    HAS_BROWSER_COOKIES = False
    get_browser_cookies = None

# æ§åˆ¶å°æ—¥å¿—çº§åˆ«è®¾ç½®ä¸ºWARNINGï¼Œå‡å°‘å¹²æ‰°
for handler in logging.root.handlers:
    if isinstance(handler, logging.StreamHandler) and not isinstance(handler, logging.FileHandler):
        handler.setLevel(logging.WARNING)

# Rich console
console = Console()


class URLExtractor:
    """URLæå–å™¨ - æ”¯æŒå¤šç§URLæ ¼å¼"""

    # URLæ¨¡å¼å®šä¹‰
    PATTERNS = {
        'video': [
            r'https?://(?:www\.)?douyin\.com/video/(\d+)',
            r'https?://(?:www\.)?douyin\.com/note/(\d+)',
            r'https?://(?:www\.)?iesdouyin\.com/share/video/(\d+)',
        ],
        'user': [
            r'https?://(?:www\.)?douyin\.com/user/([\w-]+)',
            r'https?://(?:www\.)?douyin\.com/user/\?.*sec_uid=([\w-]+)',
            r'https?://(?:www\.)?iesdouyin\.com/share/user/([\w-]+)',
        ],
        'live': [
            r'https?://live\.douyin\.com/(\d+)',
            r'https?://(?:www\.)?douyin\.com/live/(\d+)',
        ],
        'mix': [
            r'https?://(?:www\.)?douyin\.com/collection/(\d+)',
            r'https?://(?:www\.)?douyin\.com/mix/detail/(\d+)',
        ],
        'music': [
            r'https?://(?:www\.)?douyin\.com/music/(\d+)',
        ],
        'challenge': [
            r'https?://(?:www\.)?douyin\.com/challenge/(\d+)',
        ],
        'search': [
            r'https?://(?:www\.)?douyin\.com/search/([^?]+)',
        ]
    }

    @classmethod
    def extract_from_text(cls, text: str) -> List[Dict[str, str]]:
        """ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰æŠ–éŸ³é“¾æ¥"""
        urls = []

        # æå–æ‰€æœ‰URL
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+(?:[/?#][^\s]*)?'
        found_urls = re.findall(url_pattern, text)

        for url in found_urls:
            url_info = cls.parse_url(url)
            if url_info:
                urls.append(url_info)

        return urls

    @classmethod
    def parse_url(cls, url: str) -> Optional[Dict[str, str]]:
        """è§£æå•ä¸ªURL"""
        url = url.strip()

        # æ£€æµ‹URLç±»å‹
        for url_type, patterns in cls.PATTERNS.items():
            for pattern in patterns:
                match = re.match(pattern, url)
                if match:
                    return {
                        'url': url,
                        'type': url_type,
                        'id': match.group(1),
                        'original': url
                    }

        # å¤„ç†çŸ­é“¾æ¥
        if 'v.douyin.com' in url or 'dyv.im' in url:
            return {
                'url': url,
                'type': 'short',
                'id': None,
                'original': url
            }

        return None

    @classmethod
    def extract_id_from_share_text(cls, text: str) -> Optional[str]:
        """ä»åˆ†äº«æ–‡æœ¬ä¸­æå–IDï¼ˆå¤„ç†å¤åˆ¶çš„å£ä»¤ï¼‰"""
        # åŒ¹é…å½¢å¦‚ "8.43 abc:/ å¤åˆ¶æ‰“å¼€æŠ–éŸ³" çš„æ ¼å¼
        # éœ€è¦åŒ¹é…å®Œæ•´çš„çŸ­é“¾æ¥IDï¼Œä¸åªæ˜¯å‰å‡ ä¸ªå­—ç¬¦
        pattern = r'([a-zA-Z0-9]+):/'
        match = re.search(pattern, text)
        if match:
            # è¿”å›å®Œæ•´çš„ID
            full_id = match.group(1)
            # éªŒè¯IDé•¿åº¦ï¼ˆé€šå¸¸æ˜¯10-15ä¸ªå­—ç¬¦ï¼‰
            if len(full_id) >= 10:
                return full_id

        # åŒ¹é…æŠ–éŸ³å·
        pattern = r'@([a-zA-Z0-9_]+)'
        match = re.search(pattern, text)
        if match:
            return match.group(1)

        return None


class CookieHelper:
    """CookieåŠ©æ‰‹ - æä¾›å¤šç§Cookieè·å–æ–¹å¼"""

    @staticmethod
    def get_cookie_menu() -> str:
        """æ˜¾ç¤ºCookieè·å–èœå•"""
        console.print("\n[bold cyan]ğŸª Cookieè·å–æ–¹å¼[/bold cyan]")
        console.print("1. è‡ªåŠ¨ä»æµè§ˆå™¨æå–ï¼ˆæ¨èï¼‰")
        console.print("2. ä½¿ç”¨Playwrightè‡ªåŠ¨è·å–")
        console.print("3. æ‰‹åŠ¨è¾“å…¥Cookieå­—ç¬¦ä¸²")
        console.print("4. ä»æ–‡ä»¶åŠ è½½Cookie")
        console.print("5. ä¸ä½¿ç”¨Cookieï¼ˆæ¸¸å®¢æ¨¡å¼ï¼‰")

        choice = Prompt.ask(
            "\nè¯·é€‰æ‹©è·å–æ–¹å¼",
            choices=["1", "2", "3", "4", "5"],
            default="1"
        )

        return choice

    @staticmethod
    async def get_cookies_interactive() -> Optional[Union[str, Dict]]:
        """äº¤äº’å¼è·å–Cookie"""
        choice = CookieHelper.get_cookie_menu()

        if choice == "1":
            # ä»æµè§ˆå™¨æå–
            browser = Prompt.ask(
                "é€‰æ‹©æµè§ˆå™¨",
                choices=["chrome", "edge", "firefox", "brave"],
                default="chrome"
            )

            if not HAS_BROWSER_COOKIES:
                console.print("[red]æµè§ˆå™¨Cookieæå–åŠŸèƒ½ä¸å¯ç”¨ï¼ˆCryptodomeæ¨¡å—é—®é¢˜ï¼‰[/red]")
                console.print("[yellow]è¯·ä½¿ç”¨å…¶ä»–æ–¹å¼æˆ–æ‰‹åŠ¨è¾“å…¥Cookie[/yellow]")
                return None

            try:
                console.print(f"[cyan]æ­£åœ¨ä»{browser}æå–Cookie...[/cyan]")
                cookies = get_browser_cookies(browser, '.douyin.com')

                if cookies:
                    # æ˜¾ç¤ºæå–ç»“æœ
                    console.print(f"[green]âœ… æˆåŠŸæå–{len(cookies)}ä¸ªCookie[/green]")

                    # æ£€æŸ¥å…³é”®Cookie
                    important_cookies = ['msToken', 'ttwid', 'sessionid', 'sid_guard']
                    for key in important_cookies:
                        if key in cookies:
                            console.print(f"  âœ“ {key}: {cookies[key][:20]}...")

                    return cookies
                else:
                    console.print("[yellow]æœªèƒ½æå–åˆ°Cookie[/yellow]")

            except Exception as e:
                console.print(f"[red]æå–å¤±è´¥: {e}[/red]")

        elif choice == "2":
            # Playwrightè‡ªåŠ¨è·å–
            try:
                console.print("[cyan]æ­£åœ¨å¯åŠ¨æµè§ˆå™¨è‡ªåŠ¨è·å–Cookie...[/cyan]")
                async with AutoCookieManager(cookie_file='cookies.pkl', headless=False) as cm:
                    cookies_list = await cm.get_cookies()
                    if cookies_list:
                        # è½¬æ¢ä¸ºå­—å…¸
                        cookie_dict = {c['name']: c['value'] for c in cookies_list if 'name' in c and 'value' in c}
                        console.print(f"[green]âœ… æˆåŠŸè·å–{len(cookie_dict)}ä¸ªCookie[/green]")
                        return cookie_dict

            except Exception as e:
                console.print(f"[red]è·å–å¤±è´¥: {e}[/red]")

        elif choice == "3":
            # æ‰‹åŠ¨è¾“å…¥
            console.print("\n[dim]ç¤ºä¾‹: msToken=xxx; ttwid=yyy; sessionid=zzz[/dim]")
            cookie_str = Prompt.ask("è¯·è¾“å…¥Cookieå­—ç¬¦ä¸²")
            return cookie_str

        elif choice == "4":
            # ä»æ–‡ä»¶åŠ è½½
            file_path = Prompt.ask("è¯·è¾“å…¥Cookieæ–‡ä»¶è·¯å¾„", default="cookies.txt")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    # å°è¯•è§£æä¸ºJSON
                    try:
                        return json.loads(content)
                    except:
                        return content
            except Exception as e:
                console.print(f"[red]è¯»å–æ–‡ä»¶å¤±è´¥: {e}[/red]")

        elif choice == "5":
            # ä¸ä½¿ç”¨Cookie
            console.print("[yellow]å°†ä»¥æ¸¸å®¢æ¨¡å¼è¿è¡Œï¼ˆéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™ï¼‰[/yellow]")
            return None

        return None


class EnhancedDownloader:
    """å¢å¼ºç‰ˆä¸‹è½½å™¨"""

    def __init__(self, config_path: str = "config_downloader.yml"):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        self.config = self._load_config(config_path)
        self.urls_helper = Urls()
        self.result_helper = Result()
        self.utils = Utils()

        # åˆå§‹åŒ–ç»„ä»¶
        self.rate_limiter = RateLimiter(max_per_second=2)
        self.retry_manager = RetryManager(max_retries=3)

        # ç”Ÿæˆå¿…è¦çš„token
        self.mstoken = self._generate_mstoken()
        self.device_id = self._generate_device_id()

        # Cookieç®¡ç† - ä»é…ç½®æ–‡ä»¶è¯»å–
        self.cookies = self.config.get('cookies') if 'cookies' in self.config else self.config.get('cookie')
        self.headers = {**douyin_headers}
        self.headers['accept-encoding'] = 'gzip, deflate'

        # å¦‚æœé…ç½®äº†Cookieï¼Œç«‹å³è®¾ç½®
        if self.cookies:
            self._apply_cookies_to_headers()

        # æ•°æ®åº“å’Œæ—¥å¿—
        self.enable_database = bool(self.config.get('database', True))
        self.db = DataBase() if self.enable_database else None

        # ä¿å­˜è·¯å¾„
        self.save_path = Path(self.config.get('path', './Downloaded'))
        self.save_path.mkdir(parents=True, exist_ok=True)

        # ä¸‹è½½æ—¥å¿—è®°å½•å™¨
        self.download_logger = DownloadLogger(str(self.save_path))

        # URLæå–å™¨
        self.url_extractor = URLExtractor()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'start_time': time.time()
        }

    def _load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(config_path):
            logger.warning(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}

        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f) or {}
            logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return config

    def _generate_mstoken(self) -> str:
        """ç”ŸæˆmsToken"""
        import random
        import string
        charset = string.ascii_letters + string.digits + '-_='
        base_length = random.randint(100, 110)
        mstoken = ''.join(random.choice(charset) for _ in range(base_length))
        logger.info(f"ç”ŸæˆmsToken: {mstoken[:20]}...")
        return mstoken

    def _generate_device_id(self) -> str:
        """ç”Ÿæˆè®¾å¤‡ID"""
        import random
        device_id = ''.join([str(random.randint(0, 9)) for _ in range(19)])
        logger.info(f"ç”Ÿæˆè®¾å¤‡ID: {device_id}")
        return device_id

    def _apply_cookies_to_headers(self):
        """åº”ç”¨Cookieåˆ°è¯·æ±‚å¤´"""
        cookie_str = self._build_cookie_string()
        if cookie_str:
            # ç¡®ä¿cookieå­—ç¬¦ä¸²å¯ä»¥è¢«latin-1ç¼–ç 
            try:
                cookie_str.encode('latin-1')
            except UnicodeEncodeError:
                logger.warning("CookieåŒ…å«éASCIIå­—ç¬¦ï¼Œæ­£åœ¨è¿›è¡Œå®‰å…¨ç¼–ç ...")
                # é‡æ–°æ„å»ºå®‰å…¨çš„cookieå­—ç¬¦ä¸²
                cookie_str = self._build_cookie_string()

            self.headers['Cookie'] = cookie_str
            # æ›´æ–°å…¨å±€headers
            from apiproxy.douyin import douyin_headers
            douyin_headers['Cookie'] = cookie_str
            logger.info("Cookieå·²è®¾ç½®")

    async def initialize_cookies(self, cookies=None):
        """åˆå§‹åŒ–Cookie"""
        if cookies:
            self.cookies = cookies
            self._apply_cookies_to_headers()
            console.print("[green]âœ… Cookieè®¾ç½®æˆåŠŸ[/green]")
        elif not self.cookies:
            # å¦‚æœæ²¡æœ‰é…ç½®Cookieï¼Œå°è¯•ä»é…ç½®è·å–
            self.cookies = self.config.get('cookies') or self.config.get('cookie')
            if self.cookies:
                self._apply_cookies_to_headers()
                console.print("[green]âœ… Cookieä»é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ[/green]")

    def _build_cookie_string(self) -> str:
        """æ„å»ºCookieå­—ç¬¦ä¸²"""
        def safe_encode(value):
            """å®‰å…¨ç¼–ç cookieå€¼ï¼Œå¤„ç†éASCIIå­—ç¬¦"""
            if not value:
                return ''
            # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºUTF-8ç¼–ç ï¼Œç„¶åè½¬æ¢ä¸ºASCIIå®‰å…¨çš„å­—ç¬¦ä¸²
            if isinstance(value, str):
                try:
                    # å°è¯•ä½¿ç”¨latin-1ç¼–ç 
                    value.encode('latin-1')
                    return value
                except UnicodeEncodeError:
                    # å¦‚æœåŒ…å«éASCIIå­—ç¬¦ï¼Œä½¿ç”¨URLç¼–ç 
                    import urllib.parse
                    return urllib.parse.quote(value, safe='')
            return str(value)

        if isinstance(self.cookies, str):
            # å¤„ç†å­—ç¬¦ä¸²å½¢å¼çš„cookies
            cookie_pairs = []
            for pair in self.cookies.split(';'):
                pair = pair.strip()
                if '=' in pair:
                    key, value = pair.split('=', 1)
                    cookie_pairs.append(f'{key.strip()}={safe_encode(value.strip())}')
                elif pair:
                    cookie_pairs.append(pair)
            return '; '.join(cookie_pairs)
        elif isinstance(self.cookies, dict):
            return '; '.join([f'{k}={safe_encode(v)}' for k, v in self.cookies.items()])
        elif isinstance(self.cookies, list):
            kv = {c.get('name'): safe_encode(c.get('value')) for c in self.cookies if c.get('name') and c.get('value')}
            return '; '.join([f'{k}={v}' for k, v in kv.items()])
        return ''

    async def download_from_url(self, url: str, progress=None, task_id=None) -> bool:
        """æ ¹æ®URLç±»å‹è‡ªåŠ¨é€‰æ‹©ä¸‹è½½æ–¹å¼"""
        try:
            # è§£æURL
            url_info = self.url_extractor.parse_url(url)

            if not url_info:
                # å°è¯•è§£æä¸ºåˆ†äº«æ–‡æœ¬
                share_id = self.url_extractor.extract_id_from_share_text(url)
                if share_id:
                    # æ„é€ ä¸ºçŸ­é“¾æ¥
                    url = f"https://v.douyin.com/{share_id}/"
                    url_info = {'url': url, 'type': 'short', 'id': share_id}
                else:
                    logger.error(f"æ— æ³•è¯†åˆ«çš„URLæ ¼å¼: {url}")
                    return False

            # å¤„ç†çŸ­é“¾æ¥
            if url_info['type'] == 'short':
                resolved_url = await self.resolve_short_url(url_info['url'])
                url_info = self.url_extractor.parse_url(resolved_url)
                if not url_info:
                    logger.error(f"çŸ­é“¾æ¥è§£æå¤±è´¥: {url}")
                    return False

            # æ ¹æ®ç±»å‹è°ƒç”¨å¯¹åº”çš„ä¸‹è½½æ–¹æ³•
            if url_info['type'] in ['video', 'note']:
                return await self.download_single_video(url_info['url'], url_info['id'], progress, task_id)
            elif url_info['type'] == 'user':
                return await self.download_user_page(url_info['url'], url_info['id'])
            elif url_info['type'] == 'mix':
                return await self.download_mix(url_info['url'], url_info['id'])
            elif url_info['type'] == 'music':
                return await self.download_music(url_info['url'], url_info['id'])
            elif url_info['type'] == 'live':
                console.print("[yellow]ç›´æ’­ä¸‹è½½åŠŸèƒ½å¼€å‘ä¸­...[/yellow]")
                return False
            elif url_info['type'] == 'challenge':
                console.print("[yellow]è¯é¢˜ä¸‹è½½åŠŸèƒ½å¼€å‘ä¸­...[/yellow]")
                return False
            else:
                logger.error(f"ä¸æ”¯æŒçš„URLç±»å‹: {url_info['type']}")
                return False

        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
            traceback.print_exc()
            return False

    async def resolve_short_url(self, url: str) -> str:
        """è§£æçŸ­é“¾æ¥"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }

            # ä½¿ç”¨requestså¤„ç†é‡å®šå‘
            response = requests.get(url, headers=headers, allow_redirects=False, timeout=10)

            # é€æ­¥è·Ÿè¸ªé‡å®šå‘
            redirect_count = 0
            max_redirects = 5
            current_url = url

            while redirect_count < max_redirects:
                if response.status_code in [301, 302, 303, 307, 308]:
                    location = response.headers.get('Location', '')
                    if location:
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        if location.startswith('/'):
                            parsed = urlparse(current_url)
                            location = f"{parsed.scheme}://{parsed.netloc}{location}"

                        current_url = location

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«è§†é¢‘ID - æ”¹è¿›çš„æ¨¡å¼åŒ¹é…
                        if '/share/video/' in location:
                            # ä»åˆ†äº«é“¾æ¥æå–è§†é¢‘ID
                            video_id_match = re.search(r'/share/video/(\d+)', location)
                            if video_id_match:
                                video_id = video_id_match.group(1)
                                video_url = f"https://www.douyin.com/video/{video_id}"
                                logger.info(f"è§£æçŸ­é“¾æ¥æˆåŠŸï¼ˆåˆ†äº«é“¾æ¥ï¼‰: {url} -> {video_url}")
                                return video_url
                        elif '/video/' in location or '/note/' in location:
                            logger.info(f"è§£æçŸ­é“¾æ¥æˆåŠŸ: {url} -> {location}")
                            return location
                        elif '/user/' in location or 'sec_uid=' in location:
                            # å¤„ç†ç”¨æˆ·ä¸»é¡µé“¾æ¥
                            logger.info(f"è§£æçŸ­é“¾æ¥æˆåŠŸï¼ˆç”¨æˆ·ä¸»é¡µï¼‰: {url} -> {location}")
                            return location
                        elif 'modal_id=' in location:
                            # ä»modal_idæå–
                            modal_match = re.search(r'modal_id=(\d+)', location)
                            if modal_match:
                                video_id = modal_match.group(1)
                                video_url = f"https://www.douyin.com/video/{video_id}"
                                logger.info(f"è§£æçŸ­é“¾æ¥æˆåŠŸï¼ˆmodal_idï¼‰: {url} -> {video_url}")
                                return video_url

                        # ç»§ç»­è·Ÿéšé‡å®šå‘
                        response = requests.get(location, headers=headers, allow_redirects=False, timeout=10)
                        redirect_count += 1
                    else:
                        break
                else:
                    # æœ€ç»ˆURL
                    final_url = response.url if hasattr(response, 'url') else current_url

                    # æ£€æŸ¥æœ€ç»ˆURLæ˜¯å¦æ˜¯ç”¨æˆ·ä¸»é¡µ
                    if '/user/' in final_url or 'sec_uid=' in final_url:
                        logger.info(f"è§£æçŸ­é“¾æ¥æˆåŠŸï¼ˆæœ€ç»ˆç”¨æˆ·ä¸»é¡µï¼‰: {url} -> {final_url}")
                        return final_url

                    # å°è¯•ä»é¡µé¢å†…å®¹æå–
                    if response.text:
                        # æå–è§†é¢‘ID
                        video_id_match = re.search(r'/video/(\d+)', response.text)
                        if video_id_match:
                            video_id = video_id_match.group(1)
                            video_url = f"https://www.douyin.com/video/{video_id}"
                            logger.info(f"ä»é¡µé¢æå–è§†é¢‘ID: {url} -> {video_url}")
                            return video_url

                    return final_url

            return url

        except Exception as e:
            logger.warning(f"è§£æçŸ­é“¾æ¥å¤±è´¥: {e}")
            return url

    async def download_single_video(self, url: str, video_id: str = None, progress=None, task_id=None) -> bool:
        """ä¸‹è½½å•ä¸ªè§†é¢‘/å›¾æ–‡ - ä¼˜åŒ–ç‰ˆ"""
        try:
            # å¦‚æœæ²¡æœ‰video_idï¼Œå°è¯•æå–
            if not video_id:
                video_id = self._extract_video_id(url)

            if not video_id:
                logger.error(f"æ— æ³•æå–è§†é¢‘ID: {url}")
                return False

            # æ›´æ–°è¿›åº¦
            if progress and task_id is not None:
                progress.update(task_id, description="[yellow]è·å–è§†é¢‘ä¿¡æ¯...[/yellow]")

            # è·å–è§†é¢‘ä¿¡æ¯ - ä½¿ç”¨å¤šç§æ–¹æ³•å°è¯•
            video_info = await self._get_video_info_with_fallback(video_id)

            if not video_info:
                logger.error(f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {video_id}")
                self.stats['failed'] += 1
                return False

            # ä¸‹è½½åª’ä½“æ–‡ä»¶
            if progress and task_id is not None:
                desc = video_info.get('desc', 'æ— æ ‡é¢˜')[:30]
                media_type = 'å›¾æ–‡' if video_info.get('images') else 'è§†é¢‘'
                progress.update(task_id, description=f"[cyan]ä¸‹è½½{media_type}: {desc}[/cyan]")

            success = await self._download_media_files(video_info, progress, task_id)

            if success:
                self.stats['success'] += 1
                logger.info(f"ä¸‹è½½æˆåŠŸ: {url}")
                self.download_logger.add_success({
                    "url": url,
                    "title": video_info.get('desc', 'æ— æ ‡é¢˜'),
                    "video_id": video_id,
                    "file_path": str(self.save_path)
                })
            else:
                self.stats['failed'] += 1
                logger.error(f"ä¸‹è½½å¤±è´¥: {url}")
                self.download_logger.add_failure({
                    "url": url,
                    "video_id": video_id,
                    "error_message": "ä¸‹è½½åª’ä½“æ–‡ä»¶å¤±è´¥"
                })

            return success

        except Exception as e:
            logger.error(f"ä¸‹è½½è§†é¢‘å¼‚å¸¸: {e}")
            self.stats['failed'] += 1
            return False
        finally:
            self.stats['total'] += 1

    def _extract_video_id(self, url: str) -> Optional[str]:
        """æå–è§†é¢‘ID"""
        patterns = [
            r'/video/(\d+)',
            r'/note/(\d+)',
            r'modal_id=(\d+)',
            r'aweme_id=(\d+)',
            r'item_id=(\d+)',
            r'/(\d{15,20})',  # ç›´æ¥çš„æ•°å­—ID
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        return None

    async def _get_video_info_with_fallback(self, video_id: str) -> Optional[Dict]:
        """è·å–è§†é¢‘ä¿¡æ¯ - å¸¦å¤šç§é™çº§ç­–ç•¥"""

        # æ–¹æ³•1: ä½¿ç”¨ç°æœ‰çš„Douyinç±»
        try:
            from apiproxy.douyin.douyin import Douyin
            dy = Douyin(database=False)

            # è®¾ç½®Cookie
            if self.cookies:
                cookie_str = self._build_cookie_string()
                if cookie_str:
                    from apiproxy.douyin import douyin_headers
                    # ç¡®ä¿cookieå­—ç¬¦ä¸²å¯ä»¥è¢«latin-1ç¼–ç 
                    try:
                        cookie_str.encode('latin-1')
                    except UnicodeEncodeError:
                        logger.warning("CookieåŒ…å«éASCIIå­—ç¬¦ï¼Œå·²è¿›è¡Œå®‰å…¨ç¼–ç ")
                    douyin_headers['Cookie'] = cookie_str

            result = dy.getAwemeInfo(video_id)
            if result:
                logger.info(f"æ–¹æ³•1æˆåŠŸ: è·å–åˆ°è§†é¢‘ä¿¡æ¯")
                return result

        except Exception as e:
            logger.debug(f"æ–¹æ³•1å¤±è´¥: {e}")

        # æ–¹æ³•2: ä½¿ç”¨å®˜æ–¹APIå¸¦ç­¾å
        try:
            params = self._build_api_params(video_id)
            x_bogus = get_x_bogus(params, self.headers.get('User-Agent'))
            api_url = f"https://www.douyin.com/aweme/v1/web/aweme/detail/?{params}&X-Bogus={x_bogus}"

            headers = {**self.headers}
            if self.cookies:
                cookie_str = self._build_cookie_string()
                if cookie_str and 'msToken=' not in cookie_str:
                    cookie_str += f'; msToken={self.mstoken}'
                # ç¡®ä¿cookieå­—ç¬¦ä¸²å¯ä»¥è¢«latin-1ç¼–ç 
                try:
                    cookie_str.encode('latin-1')
                except UnicodeEncodeError:
                    logger.warning("CookieåŒ…å«éASCIIå­—ç¬¦ï¼Œå·²è¿›è¡Œå®‰å…¨ç¼–ç ")
                headers['Cookie'] = cookie_str

            async with aiohttp.ClientSession() as session:
                async with session.get(api_url, headers=headers, timeout=15) as response:
                    if response.status == 200:
                        data = await response.json()
                        if 'aweme_detail' in data:
                            logger.info(f"æ–¹æ³•2æˆåŠŸ: å®˜æ–¹APIè·å–æˆåŠŸ")
                            return data['aweme_detail']

        except Exception as e:
            logger.debug(f"æ–¹æ³•2å¤±è´¥: {e}")

        # æ–¹æ³•3: ä½¿ç”¨å¤‡ç”¨API
        try:
            api_url = f"https://www.iesdouyin.com/web/api/v2/aweme/iteminfo/?item_ids={video_id}"

            async with aiohttp.ClientSession() as session:
                async with session.get(api_url, headers=self.headers, timeout=15) as response:
                    if response.status == 200:
                        data = await response.json()
                        if 'item_list' in data and data['item_list']:
                            logger.info(f"æ–¹æ³•3æˆåŠŸ: å¤‡ç”¨APIè·å–æˆåŠŸ")
                            return data['item_list'][0]

        except Exception as e:
            logger.debug(f"æ–¹æ³•3å¤±è´¥: {e}")

        logger.error(f"æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œæ— æ³•è·å–è§†é¢‘ä¿¡æ¯: {video_id}")
        return None

    def _build_api_params(self, aweme_id: str) -> str:
        """æ„å»ºAPIå‚æ•°"""
        params = [
            f'aweme_id={aweme_id}',
            'device_platform=webapp',
            'aid=6383',
            'channel=channel_pc_web',
            'pc_client_type=1',
            'version_code=170400',
            'version_name=17.4.0',
            'cookie_enabled=true',
            'screen_width=1920',
            'screen_height=1080',
            'browser_language=zh-CN',
            'browser_platform=MacIntel',
            'browser_name=Chrome',
            'browser_version=122.0.0.0',
            'browser_online=true',
            f'msToken={self.mstoken}',
            f'device_id={self.device_id}',
        ]
        return '&'.join(params)

    async def _download_media_files(self, video_info: Dict, progress=None, task_id=None) -> bool:
        """ä¸‹è½½åª’ä½“æ–‡ä»¶"""
        try:
            # åˆ¤æ–­ç±»å‹
            is_image = bool(video_info.get('images'))

            # æ„å»ºä¿å­˜è·¯å¾„
            author_name = video_info.get('author', {}).get('nickname', 'unknown')
            desc = video_info.get('desc', '')[:50].replace('/', '_').replace('\n', ' ')

            # å¤„ç†æ—¶é—´æˆ³
            create_time = video_info.get('create_time', time.time())
            if isinstance(create_time, (int, float)):
                dt = datetime.fromtimestamp(create_time)
            else:
                dt = datetime.now()

            create_time_str = dt.strftime('%Y%m%d_%H%M%S')

            # åˆ›å»ºæ–‡ä»¶å¤¹
            folder_name = f"{create_time_str}_{desc}" if desc else create_time_str
            save_dir = self.save_path / author_name / folder_name
            save_dir.mkdir(parents=True, exist_ok=True)

            success = True

            if is_image:
                # ä¸‹è½½å›¾æ–‡
                images = video_info.get('images', [])
                total = len(images)

                for i, img in enumerate(images, 1):
                    if progress and task_id is not None:
                        progress.update(task_id,
                                      description=f"[cyan]ä¸‹è½½å›¾ç‰‡ {i}/{total}[/cyan]",
                                      completed=(i-1)*100/total)

                    img_url = self._get_best_quality_url(img.get('url_list', []))
                    if img_url:
                        file_path = save_dir / f"image_{i}.jpg"
                        if not await self._download_file(img_url, file_path):
                            success = False

            else:
                # ä¸‹è½½è§†é¢‘
                video_url = self._get_no_watermark_url(video_info)
                if video_url:
                    file_path = save_dir / f"{folder_name}.mp4"
                    if progress and task_id is not None:
                        progress.update(task_id, description=f"[cyan]ä¸‹è½½è§†é¢‘[/cyan]")

                    if not await self._download_file_with_progress(video_url, file_path, progress, task_id):
                        success = False

                # ä¸‹è½½éŸ³é¢‘
                if self.config.get('music', True):
                    music_url = self._get_music_url(video_info)
                    if music_url:
                        file_path = save_dir / f"{folder_name}_music.mp3"
                        await self._download_file(music_url, file_path)

            # ä¸‹è½½å°é¢
            if self.config.get('cover', True):
                cover_url = self._get_cover_url(video_info)
                if cover_url:
                    file_path = save_dir / f"{folder_name}_cover.jpg"
                    await self._download_file(cover_url, file_path)

            # ä¿å­˜JSON
            if self.config.get('json', True):
                json_path = save_dir / f"{folder_name}_data.json"
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(video_info, f, ensure_ascii=False, indent=2)

            if progress and task_id is not None:
                progress.update(task_id, completed=100, description="[green]âœ“ å®Œæˆ[/green]")

            return success

        except Exception as e:
            logger.error(f"ä¸‹è½½åª’ä½“æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def _get_no_watermark_url(self, video_info: Dict) -> Optional[str]:
        """è·å–æ— æ°´å°è§†é¢‘URL"""
        try:
            # ä¼˜å…ˆä½¿ç”¨play_addr
            play_addr = video_info.get('video', {}).get('play_addr') or \
                       video_info.get('video', {}).get('play_addr_h264')

            if play_addr:
                url_list = play_addr.get('url_list', [])
                if url_list:
                    # æ›¿æ¢ä¸ºæ— æ°´å°ç‰ˆæœ¬
                    url = url_list[0].replace('playwm', 'play')
                    return url

            # å¤‡ç”¨download_addr
            download_addr = video_info.get('video', {}).get('download_addr')
            if download_addr:
                url_list = download_addr.get('url_list', [])
                if url_list:
                    return url_list[0].replace('playwm', 'play')

        except Exception as e:
            logger.error(f"è·å–æ— æ°´å°URLå¤±è´¥: {e}")

        return None

    def _get_best_quality_url(self, url_list: List[str]) -> Optional[str]:
        """è·å–æœ€é«˜è´¨é‡çš„URL"""
        if not url_list:
            return None

        # ä¼˜å…ˆé€‰æ‹©åŒ…å«ç‰¹å®šå…³é”®è¯çš„URL
        for keyword in ['1080', 'origin', 'high']:
            for url in url_list:
                if keyword in url:
                    return url

        return url_list[0]

    def _get_music_url(self, video_info: Dict) -> Optional[str]:
        """è·å–éŸ³ä¹URL"""
        try:
            music = video_info.get('music', {})
            play_url = music.get('play_url', {})
            url_list = play_url.get('url_list', [])
            return url_list[0] if url_list else None
        except:
            return None

    def _get_cover_url(self, video_info: Dict) -> Optional[str]:
        """è·å–å°é¢URL"""
        try:
            cover = video_info.get('video', {}).get('cover', {})
            url_list = cover.get('url_list', [])
            return self._get_best_quality_url(url_list)
        except:
            return None

    async def _download_file(self, url: str, save_path: Path) -> bool:
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            if save_path.exists():
                logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨: {save_path.name}")
                return True

            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=self.headers, timeout=30) as response:
                    if response.status == 200:
                        content = await response.read()
                        with open(save_path, 'wb') as f:
                            f.write(content)
                        return True
                    else:
                        logger.error(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        return False

        except Exception as e:
            logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return False

    async def _download_file_with_progress(self, url: str, save_path: Path, progress=None, task_id=None) -> bool:
        """å¸¦è¿›åº¦çš„æ–‡ä»¶ä¸‹è½½"""
        try:
            if save_path.exists():
                logger.debug(f"æ–‡ä»¶å·²å­˜åœ¨: {save_path.name}")
                return True

            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=self.headers, timeout=60) as response:
                    if response.status == 200:
                        total_size = int(response.headers.get('content-length', 0))
                        chunk_size = 8192
                        downloaded = 0

                        with open(save_path, 'wb') as f:
                            async for chunk in response.content.iter_chunked(chunk_size):
                                f.write(chunk)
                                downloaded += len(chunk)

                                if progress and task_id is not None and total_size > 0:
                                    progress.update(task_id, completed=downloaded * 100 / total_size)

                        return True
                    else:
                        logger.error(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        return False

        except Exception as e:
            logger.error(f"ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return False

    async def download_user_page(self, url: str, user_id: str = None) -> bool:
        """ä¸‹è½½ç”¨æˆ·ä¸»é¡µä½œå“"""
        try:
            console.print("[cyan]æ­£åœ¨è·å–ç”¨æˆ·ä¿¡æ¯...[/cyan]")

            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„user_idå‚æ•°
            sec_uid = user_id

            # å¦‚æœæ²¡æœ‰ä¼ å…¥user_idï¼Œåˆ™ä»URLè§£æ
            if not sec_uid:
                # å¦‚æœæ˜¯çŸ­é“¾æ¥ï¼Œå…ˆè·å–é‡å®šå‘åçš„URL
                if 'v.douyin.com' in url:
                    resolved_url = await self.resolve_short_url(url)
                    url = resolved_url

                # ä»URLæå–sec_uid
                import re
                patterns = [
                    r'sec_uid=([\w-]+)',
                    r'/user/([\w-]+)',
                    r'/share/user/([\w-]+)'
                ]

                for pattern in patterns:
                    match = re.search(pattern, url)
                    if match:
                        sec_uid = match.group(1)
                        break

                if not sec_uid:
                    logger.error(f"æ— æ³•ä»URLæå–ç”¨æˆ·ID: {url}")
                    return False

            console.print(f"[green]ç”¨æˆ·ID: {sec_uid}[/green]")

            # è·å–ç”¨æˆ·ä½œå“åˆ—è¡¨
            videos = await self._get_user_posts(sec_uid)

            if not videos:
                console.print("[yellow]æœªæ‰¾åˆ°ç”¨æˆ·ä½œå“[/yellow]")
                return False

            console.print(f"[green]æ‰¾åˆ° {len(videos)} ä¸ªä½œå“[/green]")

            # è¯¢é—®ä¸‹è½½æ•°é‡
            max_count = min(len(videos), 10)  # é»˜è®¤æœ€å¤šä¸‹è½½10ä¸ª
            console.print(f"\n[cyan]å‡†å¤‡ä¸‹è½½å‰ {max_count} ä¸ªä½œå“[/cyan]")

            # åˆ›å»ºè¿›åº¦æ¡
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(bar_width=40),
                TaskProgressColumn(),
                TimeRemainingColumn(),
                console=console
            ) as progress:

                task = progress.add_task(
                    f"ä¸‹è½½ç”¨æˆ·ä½œå“ [0/{max_count}]",
                    total=max_count
                )

                success_count = 0
                for i, video_info in enumerate(videos[:max_count], 1):
                    progress.update(task, description=f"ä¸‹è½½ç”¨æˆ·ä½œå“ [{i}/{max_count}]")

                    # ç›´æ¥ä½¿ç”¨å·²è·å–çš„è§†é¢‘ä¿¡æ¯ä¸‹è½½ï¼Œä¸å†è°ƒç”¨å•ä¸ªè§†é¢‘API
                    if video_info and isinstance(video_info, dict):
                        # æ›´æ–°è¿›åº¦
                        desc = video_info.get('desc', 'æ— æ ‡é¢˜')[:30]
                        media_type = 'å›¾æ–‡' if video_info.get('images') else 'è§†é¢‘'
                        progress.update(task, description=f"[cyan]ä¸‹è½½{media_type}: {desc}[/cyan]")

                        # ç›´æ¥ä¸‹è½½åª’ä½“æ–‡ä»¶
                        success = await self._download_media_files(video_info)
                        if success:
                            success_count += 1
                            video_id = video_info.get('aweme_id', 'unknown')
                            logger.info(f"æˆåŠŸä¸‹è½½è§†é¢‘: {video_id}")
                            self.stats['success'] += 1
                        else:
                            self.stats['failed'] += 1
                    else:
                        # å¦‚æœè§†é¢‘ä¿¡æ¯æ— æ•ˆï¼Œå°è¯•ä½¿ç”¨æ—§æ–¹æ³•
                        video_id = video_info.get('aweme_id') or video_info.get('video_id')
                        if video_id:
                            video_url = f"https://www.douyin.com/video/{video_id}"
                            success = await self.download_single_video(video_url, video_id)
                            if success:
                                success_count += 1

                    progress.update(task, completed=i)
                    self.stats['total'] += 1

                    # é€Ÿç‡é™åˆ¶
                    await self.rate_limiter.acquire()

            console.print(f"\n[green]âœ… æˆåŠŸä¸‹è½½ {success_count}/{max_count} ä¸ªä½œå“[/green]")
            return success_count > 0

        except Exception as e:
            logger.error(f"ä¸‹è½½ç”¨æˆ·é¡µé¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    async def _get_user_posts(self, sec_uid: str, max_count: int = 50) -> List[Dict]:
        """è·å–ç”¨æˆ·ä½œå“åˆ—è¡¨"""
        try:
            # æ„å»ºAPIå‚æ•°
            params = [
                f'sec_uid={sec_uid}',
                'count=20',
                'max_cursor=0',
                'aid=6383',
                'device_platform=webapp',
                'channel=channel_pc_web',
                'pc_client_type=1',
                'version_code=170400',
                'version_name=17.4.0',
                'cookie_enabled=true',
                'screen_width=1920',
                'screen_height=1080',
                'browser_language=zh-CN',
                'browser_platform=MacIntel',
                'browser_name=Chrome',
                'browser_version=122.0.0.0',
                'browser_online=true',
                f'msToken={self.mstoken}',
            ]

            params_str = '&'.join(params)

            # ç”ŸæˆX-Bogusç­¾å
            from apiproxy.douyin.auth.signature_generator import get_x_bogus
            x_bogus = get_x_bogus(params_str, self.headers.get('User-Agent'))

            # æ„å»ºå®Œæ•´URL
            api_url = f"https://www.douyin.com/aweme/v1/web/aweme/post/?{params_str}&X-Bogus={x_bogus}"

            # è®¾ç½®headers - é‡è¦ï¼šä¸åŒ…å«sessionidï¼Œé¿å…è¿”å›ç™»å½•ç”¨æˆ·çš„ä½œå“
            headers = {**self.headers}
            if self.cookies:
                cookie_str = self._build_cookie_string()
                if cookie_str:
                    # ç§»é™¤æ‰€æœ‰sessionç›¸å…³çš„cookieï¼Œåªä¿ç•™å…¶ä»–å¿…è¦çš„cookie
                    # sessionç›¸å…³çš„cookieä¼šå¯¼è‡´APIè¿”å›ç™»å½•ç”¨æˆ·çš„ä½œå“è€Œä¸æ˜¯è¯·æ±‚çš„ç”¨æˆ·
                    cookie_parts = []
                    session_keywords = ['sessionid', 'sid_guard', 'sid_tt', 'uid_tt']
                    for part in cookie_str.split(';'):
                        part = part.strip()
                        if part:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«sessionç›¸å…³çš„å…³é”®è¯
                            is_session_cookie = False
                            for keyword in session_keywords:
                                if part.startswith(keyword + '=') or part.startswith(keyword + '_'):
                                    is_session_cookie = True
                                    break

                            if not is_session_cookie:
                                cookie_parts.append(part)

                    filtered_cookie_str = '; '.join(cookie_parts)

                    # ç¡®ä¿cookieå­—ç¬¦ä¸²å¯ä»¥è¢«latin-1ç¼–ç 
                    try:
                        filtered_cookie_str.encode('latin-1')
                    except UnicodeEncodeError:
                        logger.warning("CookieåŒ…å«éASCIIå­—ç¬¦ï¼Œå·²è¿›è¡Œå®‰å…¨ç¼–ç ")

                    if filtered_cookie_str:
                        headers['Cookie'] = filtered_cookie_str
                        logger.info(f"è¯·æ±‚ç”¨æˆ·ä½œå“æ—¶ä½¿ç”¨è¿‡æ»¤åçš„Cookieï¼ˆç§»é™¤äº†sessionidï¼‰")

            # è¯·æ±‚API
            async with aiohttp.ClientSession() as session:
                async with session.get(api_url, headers=headers, timeout=15) as response:
                    if response.status == 200:
                        data = await response.json()

                        if data and 'aweme_list' in data:
                            logger.info(f"è·å–åˆ° {len(data['aweme_list'])} ä¸ªä½œå“")
                            return data['aweme_list']
                        elif data:
                            logger.warning(f"APIè¿”å›æ ¼å¼å¼‚å¸¸: {data}")
                        else:
                            logger.warning("APIè¿”å›ç©ºæ•°æ®")
                    else:
                        logger.error(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")

            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨Douyinç±»
            logger.info("å°è¯•å¤‡ç”¨æ–¹æ¡ˆè·å–ç”¨æˆ·ä½œå“...")
            from apiproxy.douyin.douyin import Douyin
            dy = Douyin(database=False)

            # è®¾ç½®Cookie - åŒæ ·éœ€è¦ç§»é™¤sessionid
            if self.cookies:
                cookie_str = self._build_cookie_string()
                if cookie_str:
                    # ç§»é™¤æ‰€æœ‰sessionç›¸å…³çš„cookieï¼Œé¿å…è¿”å›ç™»å½•ç”¨æˆ·çš„ä½œå“
                    cookie_parts = []
                    session_keywords = ['sessionid', 'sid_guard', 'sid_tt', 'uid_tt']
                    for part in cookie_str.split(';'):
                        part = part.strip()
                        if part:
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«sessionç›¸å…³çš„å…³é”®è¯
                            is_session_cookie = False
                            for keyword in session_keywords:
                                if part.startswith(keyword + '=') or part.startswith(keyword + '_'):
                                    is_session_cookie = True
                                    break

                            if not is_session_cookie:
                                cookie_parts.append(part)

                    filtered_cookie_str = '; '.join(cookie_parts)

                    from apiproxy.douyin import douyin_headers
                    # ç¡®ä¿cookieå­—ç¬¦ä¸²å¯ä»¥è¢«latin-1ç¼–ç 
                    try:
                        filtered_cookie_str.encode('latin-1')
                    except UnicodeEncodeError:
                        logger.warning("CookieåŒ…å«éASCIIå­—ç¬¦ï¼Œå·²è¿›è¡Œå®‰å…¨ç¼–ç ")

                    if filtered_cookie_str:
                        douyin_headers['Cookie'] = filtered_cookie_str
                        logger.info(f"å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨è¿‡æ»¤åçš„Cookieï¼ˆç§»é™¤äº†sessionidï¼‰")

            # è·å–ç”¨æˆ·ä½œå“
            result = dy.getUserInfo(sec_uid, mode="post", count=max_count)
            if result and isinstance(result, list):
                return result

        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ä½œå“å¤±è´¥: {e}")

        return []

    async def download_mix(self, url: str, mix_id: str = None) -> bool:
        """ä¸‹è½½åˆé›†"""
        console.print("[yellow]åˆé›†ä¸‹è½½åŠŸèƒ½å¼€å‘ä¸­...[/yellow]")
        return True

    async def download_music(self, url: str, music_id: str = None) -> bool:
        """ä¸‹è½½éŸ³ä¹é¡µä½œå“"""
        console.print("[yellow]éŸ³ä¹é¡µä¸‹è½½åŠŸèƒ½å¼€å‘ä¸­...[/yellow]")
        return True

    def show_stats(self):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        elapsed = time.time() - self.stats['start_time']

        table = Table(title="ğŸ“Š ä¸‹è½½ç»Ÿè®¡", show_header=True, header_style="bold magenta")
        table.add_column("é¡¹ç›®", style="cyan", width=12)
        table.add_column("æ•°å€¼", style="green")

        table.add_row("æ€»ä»»åŠ¡æ•°", str(self.stats['total']))
        table.add_row("æˆåŠŸ", str(self.stats['success']))
        table.add_row("å¤±è´¥", str(self.stats['failed']))
        table.add_row("è·³è¿‡", str(self.stats['skipped']))

        if self.stats['total'] > 0:
            success_rate = self.stats['success'] / self.stats['total'] * 100
            table.add_row("æˆåŠŸç‡", f"{success_rate:.1f}%")

        table.add_row("ç”¨æ—¶", f"{elapsed:.1f}ç§’")

        console.print(table)


class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""
    def __init__(self, max_per_second: float = 2):
        self.max_per_second = max_per_second
        self.min_interval = 1.0 / max_per_second
        self.last_request = 0

    async def acquire(self):
        """è·å–è®¸å¯"""
        current = time.time()
        time_since_last = current - self.last_request
        if time_since_last < self.min_interval:
            await asyncio.sleep(self.min_interval - time_since_last)
        self.last_request = time.time()


class RetryManager:
    """é‡è¯•ç®¡ç†å™¨"""
    def __init__(self, max_retries: int = 3):
        self.max_retries = max_retries
        self.retry_delays = [1, 2, 5]

    async def execute_with_retry(self, func, *args, **kwargs):
        """æ‰§è¡Œå‡½æ•°å¹¶è‡ªåŠ¨é‡è¯•"""
        last_error = None
        for attempt in range(self.max_retries):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    delay = self.retry_delays[min(attempt, len(self.retry_delays) - 1)]
                    logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}, {delay}ç§’åé‡è¯•...")
                    await asyncio.sleep(delay)
        raise last_error


async def interactive_mode():
    """äº¤äº’æ¨¡å¼"""
    console.print(Panel.fit(
        "[bold cyan]ğŸ¬ æŠ–éŸ³ä¸‹è½½å™¨ v2.0 - å¢å¼ºç‰ˆ[/bold cyan]\n"
        "[dim]æ”¯æŒå¤šç§URLæ ¼å¼å’ŒCookieè·å–æ–¹å¼[/dim]",
        border_style="cyan"
    ))

    # åˆ›å»ºä¸‹è½½å™¨
    downloader = EnhancedDownloader()

    # è·å–Cookie
    console.print("\n[bold]æ­¥éª¤1: é…ç½®Cookie[/bold]")
    cookies = await CookieHelper.get_cookies_interactive()
    await downloader.initialize_cookies(cookies)

    # è·å–URL
    console.print("\n[bold]æ­¥éª¤2: è¾“å…¥ä¸‹è½½é“¾æ¥[/bold]")
    console.print("[dim]æ”¯æŒçš„æ ¼å¼:[/dim]")
    console.print("  â€¢ è§†é¢‘/å›¾æ–‡é“¾æ¥")
    console.print("  â€¢ ç”¨æˆ·ä¸»é¡µé“¾æ¥")
    console.print("  â€¢ åˆé›†/éŸ³ä¹é¡µé“¾æ¥")
    console.print("  â€¢ çŸ­é“¾æ¥/åˆ†äº«å£ä»¤")
    console.print("  â€¢ æ‰¹é‡è¾“å…¥ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")

    urls = []
    console.print("\n[dim]è¾“å…¥é“¾æ¥ï¼ˆè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:[/dim]")

    while True:
        url = input("> ").strip()
        if not url:
            break

        # è§£æURLæˆ–æ–‡æœ¬
        extracted_urls = URLExtractor.extract_from_text(url)
        if extracted_urls:
            urls.extend([u['url'] for u in extracted_urls])
            console.print(f"[green]âœ“ è¯†åˆ«åˆ° {len(extracted_urls)} ä¸ªé“¾æ¥[/green]")
        else:
            # å¯èƒ½æ˜¯åˆ†äº«æ–‡æœ¬
            urls.append(url)
            console.print(f"[yellow]æ·»åŠ : {url[:50]}...[/yellow]")

    if not urls:
        console.print("[red]æ²¡æœ‰è¾“å…¥ä»»ä½•é“¾æ¥[/red]")
        return

    # ç¡®è®¤ä¸‹è½½
    console.print(f"\n[cyan]å‡†å¤‡ä¸‹è½½ {len(urls)} ä¸ªé“¾æ¥[/cyan]")
    if not Confirm.ask("å¼€å§‹ä¸‹è½½?", default=True):
        return

    # å¼€å§‹ä¸‹è½½
    console.print("\n[bold green]â³ å¼€å§‹ä¸‹è½½...[/bold green]\n")

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(bar_width=40),
        TaskProgressColumn(),
        TimeRemainingColumn(),
        console=console
    ) as progress:

        for i, url in enumerate(urls, 1):
            task = progress.add_task(
                f"[{i}/{len(urls)}] å¤„ç†ä¸­...",
                total=100
            )

            success = await downloader.download_from_url(url, progress, task)

            if success:
                progress.update(task, description=f"[green]âœ“[/green] [{i}/{len(urls)}] å®Œæˆ")
            else:
                progress.update(task, description=f"[red]âœ—[/red] [{i}/{len(urls)}] å¤±è´¥")

    # æ˜¾ç¤ºç»Ÿè®¡
    downloader.show_stats()
    console.print("\n[bold green]âœ… ä¸‹è½½ä»»åŠ¡å®Œæˆï¼[/bold green]")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æŠ–éŸ³ä¸‹è½½å™¨ v2.0 - å¢å¼ºç‰ˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        'urls',
        nargs='*',
        help='è¦ä¸‹è½½çš„URLï¼ˆå¯ä»¥æ˜¯è§†é¢‘ã€ç”¨æˆ·ã€åˆé›†ç­‰ï¼‰'
    )

    parser.add_argument(
        '-c', '--config',
        default='config_downloader.yml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config_downloader.yml)'
    )

    parser.add_argument(
        '-o', '--output',
        default='./Downloaded',
        help='è¾“å‡ºç›®å½•'
    )

    parser.add_argument(
        '--cookie',
        help='Cookieå­—ç¬¦ä¸²æˆ–browser:chromeæ ¼å¼'
    )

    parser.add_argument(
        '-i', '--interactive',
        action='store_true',
        help='äº¤äº’æ¨¡å¼'
    )

    parser.add_argument(
        '--no-music',
        action='store_true',
        help='ä¸ä¸‹è½½éŸ³é¢‘'
    )

    parser.add_argument(
        '--no-cover',
        action='store_true',
        help='ä¸ä¸‹è½½å°é¢'
    )

    args = parser.parse_args()

    try:
        if args.interactive or not args.urls:
            # äº¤äº’æ¨¡å¼
            await interactive_mode()
        else:
            # å‘½ä»¤è¡Œæ¨¡å¼
            downloader = EnhancedDownloader(args.config)

            # è®¾ç½®é…ç½®
            downloader.save_path = Path(args.output)
            downloader.save_path.mkdir(parents=True, exist_ok=True)

            if args.no_music:
                downloader.config['music'] = False
            if args.no_cover:
                downloader.config['cover'] = False

            # å¤„ç†Cookie
            if args.cookie:
                if args.cookie.startswith('browser:'):
                    if not HAS_BROWSER_COOKIES:
                        console.print("[red]æµè§ˆå™¨Cookieæå–åŠŸèƒ½ä¸å¯ç”¨[/red]")
                        console.print("[yellow]è¯·ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„Cookieæˆ–æ‰‹åŠ¨æŒ‡å®š[/yellow]")
                    else:
                        browser = args.cookie.split(':', 1)[1]
                        cookies = get_browser_cookies(browser, '.douyin.com')
                        await downloader.initialize_cookies(cookies)
                else:
                    await downloader.initialize_cookies(args.cookie)

            # ä¸‹è½½
            console.print(f"[cyan]å¼€å§‹ä¸‹è½½ {len(args.urls)} ä¸ªé“¾æ¥...[/cyan]")

            for url in args.urls:
                console.print(f"\nå¤„ç†: {url}")
                success = await downloader.download_from_url(url)
                if success:
                    console.print("[green]âœ“ æˆåŠŸ[/green]")
                else:
                    console.print("[red]âœ— å¤±è´¥[/red]")

            # æ˜¾ç¤ºç»Ÿè®¡
            downloader.show_stats()

    except KeyboardInterrupt:
        console.print("\n[yellow]ç”¨æˆ·ä¸­æ–­[/yellow]")
    except Exception as e:
        console.print(f"\n[red]é”™è¯¯: {e}[/red]")
        traceback.print_exc()


if __name__ == '__main__':
    asyncio.run(main())